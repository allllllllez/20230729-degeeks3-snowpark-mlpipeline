{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75212108",
   "metadata": {},
   "source": [
    "# ML を動かしてみよう"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeb524b5",
   "metadata": {},
   "source": [
    "##  お題\n",
    "TPC-H にある注文データを使用し、3か月間の取引金額予測を行います。\n",
    "\n",
    "※ 本ハンズオンでは、時間に限りがあるため、特徴量選択やモデルのパラメータチューニング等は省略させていただきますが、\n",
    "実際にモデル構築を行う際は、目的に応じてご検討いただくのが良いかと思います。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219a5238",
   "metadata": {},
   "source": [
    "## 準備"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851c5cfe",
   "metadata": {},
   "source": [
    "Notebook のセクションと同じく、認証情報を入力してセッションを作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64062f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.snowpark.session import Session\n",
    "from config import connection_parameters\n",
    "\n",
    "# config.py の内容：\n",
    "# connection_parameters = {\n",
    "#     'account': '<org_name>-<account_name>',  # お使いの Snowflake アカウントの識別子\n",
    "#     'user': '<your_username>',  # Snowflake アカウントにサインインするユーザー名\n",
    "#     'password': '<your_password>',  # ユーザーのパスワード\n",
    "#     'role': 'SYSADMIN',  # ハンズオンで使用するロール。変更は不要です\n",
    "#     'database': 'DEGEEKS_HO_DB',  # ハンズオンで使用するデータベース。存在しなければ新規作成します（「gegeeks_ho_notebook.ipynb」をご参照ください）\n",
    "#     'schema': 'PUBLIC',  # ハンズオンで使用するスキーマ\n",
    "#     'warehouse': 'DEGEEKS_HO_WH'  # ハンズオンで使用するウェアハウス。存在しなければ新規作成します（「gegeeks_ho_notebook.ipynb」をご参照ください）\n",
    "# }\n",
    "\n",
    "session = Session.builder.configs(connection_parameters).create()\n",
    "print(session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f9f006",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.sql(f\"use database {connection_parameters['database']}\").collect()\n",
    "session.sql(f\"use schema {connection_parameters['schema']}\").collect()\n",
    "session.sql(f\"use warehouse {connection_parameters['warehouse']}\").collect()\n",
    "print(session.sql('select current_warehouse(), current_database(), current_schema()').collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44e9b14",
   "metadata": {},
   "source": [
    "モデル保存用のステージを作成します。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31448833",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_stage_name = 'model_stage'\n",
    "session.sql(f'create or replace stage \"{model_stage_name}\";').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb63e784",
   "metadata": {},
   "source": [
    "## 前処理\n",
    "\n",
    "### データセットを作成する\n",
    "日次で注文に関する金額・個数情報をサマリしたデータを作成します。\n",
    "今回は、TPC-H のサンプルクエリとほぼ同じ項目を使用します。\n",
    "https://docs.snowflake.com/ja/user-guide/sample-data-tpch.html\n",
    "\n",
    "なお、実際に予測モデルを作成する際は、特徴量の選択やデータクレンジングなどをしっかり行うことをお勧めします。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87462a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake.snowpark.window import Window\n",
    "import snowflake.snowpark.functions as F\n",
    "\n",
    "\n",
    "table_name = 'TPCH_SF1_ORDER_SUMMARY'\n",
    "\n",
    "# 移動平均をとるためにウインドウ関数の条件を作っておく\n",
    "# rows_between() で発行されるクエリが ROW BETWEEN <start> FOLLOWING  AND <end> なので合わせる\n",
    "window_3days = Window.order_by(F.col('ORDER_DATE').desc()).rows_between(Window.CURRENT_ROW, 2)\n",
    "window_1weeks = Window.order_by(F.col('ORDER_DATE').desc()).rows_between(Window.CURRENT_ROW, 6)\n",
    "\n",
    "df_orders = session.table(\"SNOWFLAKE_SAMPLE_DATA.TPCH_SF1.ORDERS\")\n",
    "df_lineitem = session.table(\"SNOWFLAKE_SAMPLE_DATA.TPCH_SF1.LINEITEM\")\n",
    "df = df_orders.join(\n",
    "    df_lineitem,\n",
    "    df_orders.col('O_ORDERKEY') == df_lineitem.col('L_ORDERKEY'),\n",
    "    how='inner'\n",
    ").with_column_renamed(\n",
    "    F.col('O_ORDERDATE'), 'ORDER_DATE'\n",
    ").group_by(\n",
    "    F.col('ORDER_DATE')\n",
    ").agg(\n",
    "    F.sum(F.col('L_EXTENDEDPRICE')).name('SUM_BASE_PRICE'),  # 合計価格\n",
    "    F.avg(F.col('SUM_BASE_PRICE')).over(window_3days).name('MOVING_3DAY_SUM_BASE_PRICE_AVG'),  # 合計金額の3日間移動平均\n",
    "    F.avg(F.col('SUM_BASE_PRICE')).over(window_1weeks).name('MOVING_1WEEK_SUM_BASE_PRICE_AVG'),  # 合計金額の1週間移動平均\n",
    "    F.sum(F.col('L_EXTENDEDPRICE') * (1-F.col('L_DISCOUNT'))).name('SUM_DISC_PRICE'),  # 割引合計価格\n",
    "    F.sum(F.col('L_EXTENDEDPRICE') * (1-F.col('L_DISCOUNT')) * (1+F.col('L_TAX'))).name('SUM_CHARGE'),  # 割引合計価格+税金\n",
    "    F.sum(F.col('L_QUANTITY')).name('SUM_QTY'),  # 合計数量\n",
    "    F.avg(F.col('SUM_QTY')).over(window_3days).name('MOVING_3DAY_SUM_QTY_AVG'),  # 合計数量の3日間移動平均\n",
    "    F.avg(F.col('SUM_QTY')).over(window_1weeks).name('MOVING_1WEEK_SUM_QTY_AVG'),  # 合計数量の1週間移動平均\n",
    "    F.avg(F.col('L_QUANTITY')).name('AVG_QTY'),  # 平均数量\n",
    "    F.avg(F.col('L_EXTENDEDPRICE')).name('AVG_PRICE'),  # 平均価格\n",
    "    F.avg(F.col('L_DISCOUNT')).name('AVG_DISC'),  # 平均割引\n",
    "    F.count(F.col('L_ORDERKEY')).name('COUNT_ORDER')  # 注文件数\n",
    ").order_by(\n",
    "    F.col('ORDER_DATE').asc()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a20036b",
   "metadata": {},
   "source": [
    "SQLで書くと、このような処理：\n",
    "\n",
    "<details>\n",
    "<summary>クエリ</summary>\n",
    "\n",
    "```sql\n",
    "\n",
    "-- 注文件数か金額を日別で並べる\n",
    "-- TPC-Hのサンプルクエリと同じ数値を日別集計しただけ\n",
    "-- 本当なら、注文ステータスや優先度なども見るべきじゃないかしら\n",
    "-- https://docs.snowflake.com/ja/user-guide/sample-data-tpch.html\n",
    "create or replace temporary table TPCH_SF1_ORDER_SUMMARY as\n",
    "    select -- top 1000000\n",
    "        orders.o_orderdate as order_date,\n",
    "        sum(l_extendedprice) as sum_base_price,  -- 合計価格\n",
    "        avg(sum_base_price) over (\n",
    "            order by order_date asc \n",
    "            rows between 2 preceding and current row) as moving_3day_sum_base_price_avg, -- 合計金額の3日間移動平均\n",
    "        avg(sum_base_price) over (\n",
    "            order by order_date asc \n",
    "            rows between 6 preceding and current row) as moving_week_sum_base_price_avg, -- 合計金額の7日間移動平均\n",
    "        avg(sum_base_price) over (\n",
    "            order by order_date asc \n",
    "            rows between 13 preceding and current row) as moving_2week_sum_base_price_avg, -- 合計金額の14日間移動平均\n",
    "        sum(l_extendedprice * (1-l_discount)) as sum_disc_price,  -- 割引合計価格\n",
    "        sum(l_extendedprice * (1-l_discount) * (1+l_tax)) as sum_charge, -- 割引合計価格+税金\n",
    "        sum(l_quantity) as sum_qty, -- 合計数量\n",
    "        avg(sum_qty) over (\n",
    "            order by order_date asc \n",
    "            rows between 2 preceding and current row) as moving_3day_sum_qty_avg, -- 合計数量の3日間移動平均\n",
    "        avg(sum_qty) over (\n",
    "            order by order_date asc \n",
    "            rows between 6 preceding and current row) as moving_week_sum_qty_avg, -- 合計数量の7日間移動平均\n",
    "        avg(sum_qty) over (\n",
    "            order by order_date asc \n",
    "            rows between 13 preceding and current row) as moving_2week_sum_qty_avg, -- 合計数量の14日間移動平均\n",
    "        avg(l_quantity) as avg_qty,  -- 平均数量\n",
    "        avg(l_extendedprice) as avg_price,  -- 平均合計価格\n",
    "        avg(l_discount) as avg_disc,  -- 平均割引の合計\n",
    "        count(distinct l_orderkey) as count_order  -- 注文件数\n",
    "        -- count(*) as count_lineitem -- 注文ごとのラインアイテム数合計\n",
    "    from\n",
    "        SNOWFLAKE_SAMPLE_DATA.TPCH_SF1.ORDERS as orders\n",
    "    inner join SNOWFLAKE_SAMPLE_DATA.TPCH_SF1.LINEITEM as lineitem\n",
    "        on orders.o_orderkey = lineitem.l_orderkey\n",
    "    group by\n",
    "        order_date;\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56934721",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.limit(10).to_pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda33816",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().to_pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43883efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import snowflake.snowpark.functions as F\n",
    "from matplotlib import pyplot\n",
    "\n",
    "\n",
    "df.select(F.col('ORDER_DATE'), F.col('COUNT_ORDER')).to_pandas().set_index('ORDER_DATE').plot(figsize=(15,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6add6b24",
   "metadata": {},
   "source": [
    "事前にデータセットをトレーニング用/評価用に分けます。\n",
    "TPC-H データには 1992-01-01 から 1998-08-02 のデータが含まれています(2023年2月10日現在)。\n",
    "今回は、\n",
    "\n",
    "- トレーニング用： 1992-01-01 ～ 1997-12-31\n",
    "- 評価用：1998-01-01～1998-03-31\n",
    "\n",
    "と分割します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de66f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_table_name = 'TPCH_SF1_ORDER_SUMMARY_TRAIN'\n",
    "test_table_name = 'TPCH_SF1_ORDER_SUMMARY_TEST'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa9f447",
   "metadata": {},
   "outputs": [],
   "source": [
    "import snowflake.snowpark.types as T\n",
    "\n",
    "\n",
    "df_train = df.where(F.col('ORDER_DATE') <= '1997-12-31')\n",
    "df_test = df.where(\n",
    "    (F.col('ORDER_DATE') >= '1998-01-01')\n",
    "    & (F.col('ORDER_DATE') < '1998-03-31')\n",
    ")\n",
    "\n",
    "df_train.write.mode(\"overwrite\").save_as_table(train_table_name, table_type=\"temporary\")\n",
    "df_test.write.mode(\"overwrite\").save_as_table(test_table_name, table_type=\"temporary\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e1a31c",
   "metadata": {},
   "source": [
    "時間のある方は、データを見ておきましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99db97a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.select(F.col('ORDER_DATE'), F.col('COUNT_ORDER')).to_pandas().set_index('ORDER_DATE').plot(figsize=(15,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c200d0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_test.select(F.col('ORDER_DATE'), F.col('COUNT_ORDER')).to_pandas().set_index('ORDER_DATE').plot(figsize=(15,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe51876",
   "metadata": {},
   "source": [
    "## 学習\n",
    "学習を実行してみましょう。\n",
    "今回は、ストアドプロシージャで学習の実行と学習済みモデルの保存を行います。\n",
    "<!-- 先日の Snowflake Summit 2023 にて発表された[Snowpark ML](https://docs.snowflake.com/developer-guide/snowpark-ml/index)を早速使用してみましょう！Snowpark MLは、MLモデル構築・デプロイのためのツールセットです。主要なコンポーネントとして、ML モデルを効率的に開発するための Snowpark ML Development（`snowflake.ml.modeling`）とモデル管理機能をもつ Snowpark ML Ops（プライベートプレビュー中）が含まれます。  \n",
    "今回のハンズオンでは、Snowpark ML Development を使い、データの前処理、特徴量エンジニアリング、トレーニングを行ってみましょう -->\n",
    "\n",
    "\n",
    "学習を実行する際、集計や変換などの処理より大きなメモリが必要な場合があります。その場合は Snowpark 最適化インスタンスを使うとよいでしょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88bb2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import snowflake.snowpark\n",
    "import snowflake.snowpark.types as T\n",
    "from snowflake.snowpark import FileOperation\n",
    "from snowflake.snowpark.functions import sproc\n",
    "from snowflake.snowpark.session import Session\n",
    "# from snowflake.ml.modeling.xgboost import XGBRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "import io\n",
    "import dill\n",
    "\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8fda39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def save_file(\n",
    "            session: snowflake.snowpark.Session, \n",
    "            model, \n",
    "            path: str\n",
    "        ) -> str:\n",
    "    ''' model を session で接続している環境の path に保存する'''\n",
    "    input_stream = io.BytesIO()\n",
    "    dill.dump(model, input_stream)\n",
    "    try:\n",
    "        session._conn._cursor.upload_stream(input_stream, path)\n",
    "        msg = \"successfully created file: \" + path\n",
    "    except Exception as e:\n",
    "        msg = f'upload stream no exists. ({e})'\n",
    "    \n",
    "    return msg\n",
    "\n",
    "\n",
    "\n",
    "def train_xgboost_model(\n",
    "        session: Session, \n",
    "        training_table: str,\n",
    "        feature_cols: list,\n",
    "        target_col: str,\n",
    "        model_name: str) -> T.Variant:\n",
    "    # 対象データを取得\n",
    "    local_training_data = session.table(training_table).to_pandas()\n",
    "    X = local_training_data[feature_cols]\n",
    "    y = local_training_data[target_col]\n",
    "\n",
    "    # 学習\n",
    "    xgbmodel = XGBRegressor(random_state=123)\n",
    "    xgbmodel.fit(X,y)\n",
    " \n",
    "    # 特徴量重要度を取得\n",
    "    feat_importance = pd.DataFrame(xgbmodel.feature_importances_,feature_cols,columns=['FeatImportance']).to_dict()\n",
    "\n",
    "    # モデルを内部ステージに保存\n",
    "    print(save_file(session, xgbmodel, f'@\"{model_stage_name}\"/{model_name}'))\n",
    "\n",
    "    return feat_importance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cac0e3d",
   "metadata": {},
   "source": [
    "ストアドプロシージャを登録し、実行しましょう。\n",
    "ここでは `register()` を使用ししますが、デコレータ `@register()` を使う方法もあります。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d058fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_xgboost_model_sproc = session.sproc.register(\n",
    "    train_xgboost_model,\n",
    "    stage_location=f'@\"{model_stage_name}\"',\n",
    "    packages=['snowflake-snowpark-python', 'xgboost', 'pandas', 'dill'],\n",
    "    replace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6658e678",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_table = 'TPCH_SF1_ORDER_SUMMARY_TRAIN'\n",
    "model_name = 'xgboost_model.sav'\n",
    "\n",
    "feature_cols = df_train.columns\n",
    "# target_col = 'SUM_BASE_PRICE'  # 当たりすぎる\n",
    "target_col = 'COUNT_ORDER'\n",
    "feature_cols.remove(target_col)\n",
    "feature_cols.remove('ORDER_DATE')\n",
    "\n",
    "feat_importance = train_xgboost_model_sproc(\n",
    "    training_table, \n",
    "    feature_cols,\n",
    "    target_col,\n",
    "    model_name, \n",
    "    session=session\n",
    ")\n",
    "\n",
    "print(feat_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4366c467",
   "metadata": {},
   "source": [
    "## 予測\n",
    "予測を実行します。\n",
    "UDF を作成して、Snowflake にあるテーブルに対して予測を実行し、結果を Snowflake テーブルに保存します。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a379667",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import cachetools\n",
    "import os\n",
    "import dill\n",
    "from snowflake.snowpark.functions import udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392d4cf6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "\n",
    "@cachetools.cached(cache={})\n",
    "def load_from_snowflake_import_dir(filename):\n",
    "    '''指定したモデルファイルを読み込む。\n",
    "    対象ファイルがインポートされているか、対象ファイルが存在するステージが stage_location に指定されていることが必要'''\n",
    "    import_dir = sys._xoptions.get('snowflake_import_directory')\n",
    "    with open(os.path.join(import_dir, filename), 'rb') as file:\n",
    "        m = dill.load(file)\n",
    "        return m\n",
    "\n",
    "def predict(args: list) -> float:\n",
    "    model = load_from_snowflake_import_dir(model_name)  \n",
    "    row = pd.DataFrame(\n",
    "        [args],\n",
    "        columns=feature_cols\n",
    "    )\n",
    "    return model.predict(row)\n",
    "\n",
    "\n",
    "predict_xgboost_regression_udf = session.udf.register(\n",
    "    func=predict, \n",
    "    name='predict_xgboost_regression_udf', \n",
    "    stage_location=f'@\"{model_stage_name}\"',\n",
    "    return_type = T.FloatType(),\n",
    "    replace=True, \n",
    "    is_permanent=True, \n",
    "    imports=[f'@\"{model_stage_name}\"/{model_name}'],\n",
    "    packages=['pandas', 'xgboost', 'cachetools', 'dill'], \n",
    "    session=session\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6aeaf6",
   "metadata": {},
   "source": [
    "予測を実行します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff7343c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_table_name = 'TPCH_SF1_ORDER_SUMMARY_PRED'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493bfc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df_test.select(\n",
    "      F.col('ORDER_DATE'),\n",
    "      F.col(target_col),\n",
    "      F.call_udf(\"predict_xgboost_regression_udf\", F.array_construct(*feature_cols)).alias(f'PREDICTED_{target_col}')\n",
    ").write.mode('overwrite').saveAsTable(pred_table_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9296ac1e",
   "metadata": {},
   "source": [
    "予測結果を確認してみましょう。\n",
    "\n",
    "なお、ここでは流れで Notebook 上でグラフを確認しますが、Snowsight で見ることも可能です。モデルを運用していく場合、Snowsight 上で精度確認できるようなダッシュボードを作っておくとよいかもしれません。\n",
    "\n",
    "現在プライベートプレビュー中の Streamlit in Snowflake が公開されたら、精度可視化アプリケーションを作成していつでも精度確認できるようにするのも良さそうですね。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2193ad4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pdf_score = session.table(pred_table_name).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35d1e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_score.set_index('ORDER_DATE').plot(figsize=(15,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348e79a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
